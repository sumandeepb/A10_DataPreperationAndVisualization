{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection is also known as Variable selection or Attribute selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "pt = np.get_printoptions()['threshold']\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data/pokemon.csv')\n",
    "dataframe.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_features = dataframe.iloc[:,4:12]\n",
    "pokemon_classes = dataframe['Legendary']\n",
    "pokemon_X = np.array(pokemon_features)\n",
    "pokemon_y = np.array(pokemon_classes).T\n",
    "print('Feature set shape:', pokemon_X.shape)\n",
    "print('Response class shape:', pokemon_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=.15)\n",
    "vt.fit(pokemon_X, pokemon_y)\n",
    "\n",
    "\n",
    "feature_scores = [(item, score) for item, score in zip(pokemon_features, vt.variances_)]\n",
    "sorted(feature_scores, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(pokemon_X, pokemon_y)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "importance_scores = rfc.feature_importances_\n",
    "feature_importances = [(feature, score) for feature, score in zip(pokemon_features, importance_scores)]\n",
    "sorted(feature_importances, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the variance scores Total, Sp. Atk, Attack, Defence are the top four features which decides a pokemon's ability to be legendary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
    "sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "                 .stack()\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "#Priniting the highly correlated pairs\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation map\n",
    "f,ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(corr_matrix, annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Defense-Sp. Def  , Sp. Atk-Sp. Def , Total-Legendary  are moderately correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficients.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "FS1.webp": {
     "image/webp": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FS1.webp](attachment:FS1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pearson’s Correlation:</b> It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson’s correlation is given as:\n"
   ]
  },
  {
   "attachments": {
    "FS2.webp": {
     "image/webp": "UklGRtQEAABXRUJQVlA4TMgEAAAvrwASAAUl7P/Xts3vVu4VvWtovHCZmVKmJOPNUGZmZmYGu8zMzIyhMgUNkmxZ8P8eJMVOWuu4RfR/AvAfVv3lw2Mipi9hAHie50UoPoETjXw8z4kABE7g+AbvTMPKdBWA++ek1KQ/5SkpyQn/BPRu/5SSmjhMhr9tSkI94Wria5OQVyMPWs9S6ugHHzUyNwB9xo2h4SKAlVE7vZD625k5sGcwHXgs1YvVjrNR6iyK44Hc+GwAyKmSG1kEt1cje30qAFmRwWRJgbe2E4Z2mj1zFErvb0E38SbhLLRc0uyIcjyxUfsZwIeMnm0zRExsMqDXyYPpA3rJLyjf6BZZOqohYDaNEjuegG5wQJoUQXbTwaLGlC9EjQr4RyWK7mVkC3CNHfeYi9xGweZ0F6HmkKXNbOizGSm+yOGNSwriwjKMpmIgi2aDj+0icr+UANtjvEanalJGSMEuZIfxtmruyFFAdhVAsAu5gXxyMMyirOu9VWBlEmdwJuFxHYs7FKlLteJSuKgkchSSQwEgdaESIIvsQE7VUb1PAnDW8ejldXwFO80JxWNpESjF1qqeyMG3iOOAZWdWUDawnJyA1CXmJx7ATirWeZpwB7hFKX7N7llM7yaNhrHqSOEjBx5W7ZJzqQEnde56zZncWQHgolkAUEQ3AXZ03DfNPoBts9DSkwzueDquKXQNoNFHmUHgtzQ5giDP9stf+YA0u2fvlQoAiA3ea/xNVwLiX7b+1rvw/+lwWKeowJ42WzR3/nbYrVONfNFzEFGZCF1FhX4Quqs6SQjjkBOaUK/UckeW8Hujb4XhdUM+DFyXWTCJtxsKIbEplxHGbWmiWcCFpSFBRhj5v1V8oSInf3WghBZeBV/o8V49MovKSiqlWl6+6vOjn3uTbAwBj1eA1+fWeDweFarHI+kd/yU+Pj7+F+0/SsS7XWUP5LRkAa6mNF1MSLBLwOP+1O4ZbtbMPKcnFegXFhZy0H917+v5uryNruODEJcsAA/o3PsBHLRi6h8Mc9cwlOHIn76eE8oZF5umIIvsAIKtm3f6BP0FFv7sbBjfstqMlzO9yJ1NcwBHlRwA2EiPYfiabOlqKd7tcBqfRqRfTY2ytta6CAByeu1FRnJKKxmmVE2P29hzaB606RcH1xUNArEbYU652DRFgTY48xwe0BmAzfsMvKaXJqWAZkPLVsRUuKmOrBa7AnyNMdjVjKxBc3K0Qp4OgrzAEBQ4CSiyQuJ5P8wpK0IYZ93HFypnO11O51Y1lBKXy+V0vvhqhPcpvlApk7RRLwFFUaCqqmZ2NSKiaitVVQGgqOxr9KXKnVJ2FHeu/dYHnEmq20gc2DL+ArCyynT3DDrjEX5MTTyLB4mJa8zDDjoGOKkIgPKCdmLcHIEht7oNKCQnIHybyfCxaZ5qGuS0pABgT+YAQO7R4+QcAFhJOcAJegFggUUIOt7BPPIpdsBX2wHd/dQfWnuyAJaRzAN4TfvTL8FEqo5+ijgmQdR7RBN1jsUUSQdrPgSAYOuoszCV+Y3bNEwXoftu9DiLX8NsP7dtkgct++MPZi4QKPAw6L7t9PkN7dMAxcUB6PpjN8Kkft4QdVZ9ltrqtI5xvotWKSblnmv/PXZv397jrHT3Xfu3m5X/VQk="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FS2.webp](attachment:FS2.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>LDA:</b> Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "\n",
    "<b>ANOVA:</b> ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n",
    "\n",
    "<b>Chi-Square:</b> It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n",
    "One thing that should be kept in mind is that filter methods do not remove multicollinearity. So, you must deal with multicollinearity of features as well before training models for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorical variable</h3>\n",
    "\n",
    "Categorical variables contain a finite number of categories or distinct groups. Categorical data might not have a logical order. For example, categorical predictors include gender, material type, and payment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Continuous variable</h3>\n",
    "\n",
    "Continuous variables are numeric variables that have an infinite number of values between any two values. A continuous variable can be numeric or date/time. For example, the length of a part or the date and time a payment is received.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the categorical variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_cat = [var for var in dataframe.columns if \n",
    "dataframe[var].nunique()/dataframe[var].count() < 0.05] # can take some other threshold\n",
    "\n",
    "print(\"The categorical variables are: \"+str(likely_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the continuous variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_cont = [var for var in dataframe.columns if \n",
    "dataframe[var].nunique()/dataframe[var].count() > 0.05] # can take some other threshold\n",
    "\n",
    "print(\"The continuous variables are: \"+str(likely_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms cannot work with categorical data directly. Categorical data must be converted to numbers. This applies when you are working with a sequence classification type problem and plan on using deep learning methods such as Long Short-Term Memory recurrent neural networks.\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors. <br>\n",
    "This first requires that the categorical values be <b>mapped to integer values</b>.\n",
    "Then, each integer value is represented as a binary vector that is all <b>zero</b> values <b>except the index of the integer</b>, which is marked with a 1.\n",
    "\n",
    "Lets perform a one hot encoding on our categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(dataframe,prefix=likely_cat,columns = likely_cat) #use the get_dummies() from pandas for one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recursive Feature Elimination</h1>\n",
    "\n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "\n",
    "\n",
    "The example below uses RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(pokemon_X, pokemon_y)\n",
    "selected_features = [val for idx,val in enumerate(pokemon_features.columns) if fit.support_[idx]]\n",
    "print(\"Num Features: \"+ str(fit.n_features_))  \n",
    "print(\"Selected Features: \"+ str(selected_features)) \n",
    "print(\"All Features: \"+ str(list(pokemon_features.columns)))\n",
    "print(\"Feature Ranking: \" + str(fit.ranking_))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
