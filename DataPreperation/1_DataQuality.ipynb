{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's what we're going to do on missing value treatment:\n",
    "\n",
    "### 1) Take a first look at the data\n",
    "### 2) See how many missing data points we have\n",
    "### 3) Figure out why the data is missing\n",
    "### 4) Drop missing values\n",
    "### 5) Filling in missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a first look at the data\n",
    "#### We will use a python library called pandas which is a datastructure and data analysis tool. More on pandas in the A11 course\n",
    "#### Few other libraries we will use are seaborn, matplotlib for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCopy = data.copy()\n",
    "dataCopy = dataCopy.replace('-', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missingno is a python library for nice visulaisation of missing number in the data\n",
    "import missingno as msno\n",
    "\n",
    "# Nullity or missing values by columns\n",
    "msno.matrix(df=dataCopy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So how many missing datapoints we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = dataCopy.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove incomplete rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all rows with any NA values is easy\n",
    "\n",
    "dataCopyDropAll = dataCopy.copy()\n",
    "dataCopyDropAll.dropna(inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropAll.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might drop a lot of rows which might hamper our analysis\n",
    "\n",
    "# We can drop rows with all NULLs, thats not the case here\n",
    "dataCopyDropRowWithAllNA = dataCopy.copy()\n",
    "dataCopyDropRowWithAllNA.dropna(how='all',inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropRowWithAllNA.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also can remove rows with a threshold value\n",
    "\n",
    "dataCopyDropRowWithThres = dataCopy.copy()\n",
    "dataCopyDropRowWithThres.dropna(thresh=10,inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropRowWithThres.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes it makes sense to drop \n",
    "# Drop the columns with that are all NA values\n",
    "dataCopyDropColAllNA = dataCopy.copy()\n",
    "dataCopyDropColAllNA.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropColAllNA.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns with any NA values:\n",
    "dataCopyDropColAnyNA = dataCopy.copy()\n",
    "dataCopyDropColAnyNA.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropColAnyNA.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold thing also can be applied here\n",
    "\n",
    "dataCopyDropColWithThres = dataCopy.copy()\n",
    "dataCopyDropColWithThres.dropna(axis=1, thresh=5, inplace=True)\n",
    "\n",
    "msno.matrix(df=dataCopyDropColWithThres.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is the point at which we get into the part of data science that I like to call \"data intution\", by which I mean \"really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis\". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:\n",
    "\n",
    "Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "\n",
    "If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\" and we'll learn how to do it next! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, median, mode \n",
    "### Frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple imputation to make the nan values 0, but not a good idea always\n",
    "dataCopyFillNAZero = dataCopy.copy()\n",
    "dataCopyFillNAZero = dataCopyFillNAZero.fillna(0)\n",
    "dataCopyFillNAZero.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values can also be replace with mean, media, mode or frquent values in the column\n",
    "dataCopyFillNAMean = dataCopy.copy()\n",
    "dataCopyFillNAMean[\"first_class.average\"] = pd.to_numeric(dataCopyFillNAMean[\"first_class.average\"])\n",
    "dataCopyFillNAMean = dataCopyFillNAMean.fillna(dataCopyFillNAMean.mean())\n",
    "dataCopyFillNAMean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple imputer from sklearn cane be used, Detailed sklearn will be covered in the next module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.impute.SimpleImputer(missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "\n",
    "missing_values : number, string, np.nan (default) or None\n",
    "The placeholder for the missing values. All occurrences of missing_values will be imputed.\n",
    "\n",
    "strategy : string, optional (default=”mean”)\n",
    "The imputation strategy.\n",
    "\n",
    "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.\n",
    "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "New in version 0.20: strategy=”constant” for fixed value imputation.\n",
    "\n",
    "fill_value : string or numerical value, optional (default=None)\n",
    "When strategy == “constant”, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and “missing_value” for strings or object data types.\n",
    "\n",
    "erbose : integer, optional (default=0)\n",
    "Controls the verbosity of the imputer.\n",
    "\n",
    "copy : boolean, optional (default=True)\n",
    "If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if copy=False:\n",
    "\n",
    "If X is not an array of floating values;\n",
    "If X is encoded as a CSR matrix;\n",
    "If add_indicator=True.\n",
    "add_indicator : boolean, optional (default=False)\n",
    "If True, a MissingIndicator transform will stack onto output of the imputer’s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won’t appear on the missing indicator even if there are missing values at transform/test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "dataCopyImputation = dataCopy.copy()\n",
    "my_imputer = SimpleImputer()\n",
    "dataCopyImputation['first_class.average'] = dataCopyImputation['first_class.average'].astype('float')\n",
    "dataCopyImputation['first_class.strike_rate'] = dataCopyImputation['first_class.strike_rate'].astype('float')\n",
    "data_with_imputed_values = my_imputer.fit_transform(dataCopyImputation[['first_class.average','first_class.strike_rate']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Predicting Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data.\n",
    "\n",
    "The assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables.\n",
    "\n",
    "https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment: Use KNN Impute library to fill up the missing data from data/scrapped_data_large.csv\n",
    "\n",
    "import knnimpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparametric Missing Value Imputation Using Random Forest\n",
    "'missForest' is used to impute missing values particularly in the case of mixed-type data. It can be used to impute continuous and/or categorical data including complex interactions and nonlinear relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy data removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_data(s):\n",
    "    s = s.replace('u', '')\n",
    "    s = s.replace('\\'', '')\n",
    "    s = s.upper()\n",
    "    s = s.replace(',', ' ')\n",
    "    s = s.replace('[', '')\n",
    "    s = s.replace(']', '')\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,\\$%])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|\"Â«\\n])', ' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dataCopyNoisyData = dataCopy.copy()\n",
    "dataCopyNoisyData['born_new'] = dataCopyNoisyData['born'].apply(clean_data)\n",
    "dataCopyNoisyData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inconsistent data removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fuzzy matching to correct inconsistent data entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the fuzzywuzzy package to help identify which string are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, it’s fun! :)\n",
    "\n",
    "Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "\n",
    "Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzz is used to compare TWO strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# process is used to compare a string to MULTIPLE other strings\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#fuzz.ratio compares the entire string, in order\n",
    "print(fuzz.ratio(\"this is a test\", \"this is a fun\"))\n",
    "#fuzz.partial_ratio compares subsections of the string\n",
    "print(fuzz.partial_ratio(\"this is a test\", \"test a is this\"))\n",
    "#fuzz.token_sort_ratio ignores word order\n",
    "print(fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\"))\n",
    "print(fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))\n",
    "#fuzz.token_set_ratio ignores duplicate words\n",
    "print(fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))\n",
    "\n",
    "# replace data with one consistent version if match score is higher than a pre-determined threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
