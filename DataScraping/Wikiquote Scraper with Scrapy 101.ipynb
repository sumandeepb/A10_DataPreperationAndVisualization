{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we scrape a single website?\n",
    "In this case, we don’t want to follow any links. The topic of following links I will describe in another blog post.\n",
    "\n",
    "First of all, we will use Scrapy running in Jupyter Notebook.\n",
    "Unfortunately, there is a problem with running Scrapy multiple times in Jupyter. So let’s assume for now that we can run a CrawlerProcess only once.\n",
    "### [Scrapy Spider](https://docs.scrapy.org/en/latest/topics/spiders.html)\n",
    "In the first step, we need to define a Scrapy Spider.  \n",
    "\n",
    "<div>\n",
    "<b>It consists of two essential parts:</b>\n",
    "<ul>\n",
    "    <li>start URLs (which is a list of pages to scrape)</li>\n",
    "    <li>the selector (or selectors) to extract the interesting part of a page.</li>\n",
    "</ul>\n",
    "</div>\n",
    "The selector (or selectors) to extract the interesting part of a page. In this example, we are going to extract Marilyn Manson’s quotes from Wikiquote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DijkstraQuotes(scrapy.Spider):\n",
    "    name = \"DijkstraQuotes\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Edsger_W._Dijkstra',\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DijkstraQuotes)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the source code of the page. The content is inside a div with “mw-parser-output” class. Every quote is in a “li” element. We can extract them using a CSS selector.\n",
    "\n",
    "$\n",
    "{'quote': '<li>When I came back from Munich, it was September, and I was Professor of Mathematics at the Eindhoven University of Technology. Later I learned that I had been the Department\\'s third choice, after two numerical analysts had turned the invitation down; the decision to invite me had not been an easy one, on the one hand because I had not really studied mathematics, and on the other hand because of my sandals, my beard and my \"arrogance\" (whatever that may be).\\n<ul><li>Dijkstra (1993) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.utexas.edu/users/EWD/transcriptions/EWD11xx/EWD1166.html\">\"From my Life\"</a> (EWD 1166).</li></ul></li>'}\n",
    "$\n",
    "\n",
    "It is not a perfect output. We don’t want the source of the quote and HTML tags. Let’s do it in the most trivial way because it is not a blog post about extracting text from HTML. I am going to split the quote into lines, select the first one and remove HTML tags.\n",
    "\n",
    "\n",
    "### Processing pipeline\n",
    "The proper way of doing processing of the extracted content in Scrapy is using a processing pipeline. As the input of the processor, we get the item produced by the scraper and we must produce output in the same format (for example a dictionary).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        lines = dict(item)[\"quote\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "        return {'quote': first_line}\n",
    "    \n",
    "    def __remove_html_tags__(self, text):\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to add a pipeline item. It is just a part of the custom_settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class DijkstraQuotesFirstLineOnly(scrapy.Spider):\n",
    "    name = \"DijkstraQuotesFirstLineOnly\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Edsger_W._Dijkstra',\n",
    "    ]\n",
    "    \n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {'__main__.ExtractFirstLine': 1},\n",
    "    }\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "    \n",
    "    def __remove_html_tags__(self, text):\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file\n",
    "Now we want to store the quotes in a CSV file. We can do it using a custom configuration. We need to define a feed format and the output file name.\n",
    "\n",
    "### Logging\n",
    "There is one annoying thing. Scrapy logs a vast amount of information.\n",
    "\n",
    "Fortunately, it is possible to define the log level in the settings too. We must add this line to custom_settings:\n",
    "\n",
    "$\n",
    "custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.ExtractFirstLine': 1},\n",
    "        'FEED_FORMAT':'csv',\n",
    "        'FEED_URI': 'marilyn_manson.csv'\n",
    "    }\n",
    "$\n",
    "\n",
    "Now let's have a look on the full codeblock which will generate the csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "import re\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        lines = dict(item)[\"quote\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "        return {'quote': first_line}\n",
    "    \n",
    "    def __remove_html_tags__(self, text):\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "class DijkstraQuotesToCsv(scrapy.Spider):\n",
    "    name = \"DijkstraQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Edsger_W._Dijkstra',\n",
    "    ]\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.ExtractFirstLine': 1},\n",
    "        'FEED_FORMAT':'csv',\n",
    "        'FEED_URI': 'dijkstra_quotes.csv'\n",
    "    }\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "            \n",
    "process = CrawlerProcess()\n",
    "process.crawl(DijkstraQuotesToCsv)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing CricInfo player info\n",
    "Now we can look at an example with more extensive use of the Scrapy framework (Please note that this example will take a lot of time to finish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.selector import Selector\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from scrapy.http import Request, FormRequest\n",
    "from collections import namedtuple\n",
    "import re\n",
    "import logging\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class CricInfoPlayerInfoSpider(scrapy.Spider):\n",
    "    name='cricinfo-spider'\n",
    "    start_urls=['http://www.espncricinfo.com/ci/content/player/country.html?country=6;alpha=A']\n",
    "    first_class_batting_stats=namedtuple(\"first_batting_stats\", 'matches, inns, not_outs, runs, highest, average, balls_faced, strike_rate, hundreds, fifties, boundaries, sixes, catches_taken, stumpings_made')\n",
    "    \n",
    "    def parse(self, response):\n",
    "        sel = Selector(text=response.body_as_unicode(), type=\"html\")\n",
    "        players_urlpath=sel.xpath(\n",
    "            '//div[@id=\"ciPlayerbyCharAtoZ\"]//ul//li//a/@href'\n",
    "        )\n",
    "        for url in players_urlpath.extract()[1:-1]:\n",
    "            url = \"http://www.espncricinfo.com\" + url\n",
    "            request = scrapy.Request(url, self.parse_player_names)\n",
    "            yield request\n",
    "\n",
    "    def parse_player_names(self,response):\n",
    "        sel= Selector(text=response.body_as_unicode(), type=\"html\")\n",
    "        urlPath=sel.xpath(\n",
    "            '//td[@class=\"ciPlayernames\"]//a/@href'\n",
    "        )\n",
    "\n",
    "        for url in urlPath.extract()[1:-1]:\n",
    "            url = \"http://www.espncricinfo.com\" + url\n",
    "            request = scrapy.Request(url, self.parse_player_details)\n",
    "            request.meta['url']=url\n",
    "            yield request\n",
    "\n",
    "    def parse_player_details(self, response):\n",
    "        sel = Selector(text=response.body_as_unicode(), type=\"html\")\n",
    "        name = sel.xpath(\n",
    "            '//div[@class=\"ciPlayernametxt\"]/div/h1/text()'\n",
    "        )\n",
    "\n",
    "        dob_place=sel.xpath(\n",
    "            '//p[@class=\"ciPlayerinformationtxt\"]/b[contains(text(), \"Born\")]/following-sibling::span/text()'\n",
    "        )\n",
    "\n",
    "        teams=sel.xpath(\n",
    "            '//p[@class=\"ciPlayerinformationtxt\"]/b[contains(text(), \"Major teams\")]/following-sibling::span/text()'\n",
    "        )\n",
    "        self.first_class_batting_stats.matches=sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[2]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.inns = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[3]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.not_outs = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[4]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.runs = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[5]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.highest = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[6]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.average = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[7]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.balls_faced = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[8]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.strike_rate = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[9]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.hundreds = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[10]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.fifties = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[11]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.boundaries = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[12]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.sixes = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[13]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.catches_taken = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[14]/text()'\n",
    "        )\n",
    "\n",
    "        self.first_class_batting_stats.stumpings_made = sel.xpath(\n",
    "            '//*[@id=\"ciHomeContentlhs\"]/div[4]/table[1]/tbody/tr[1]/td[15]/text()'\n",
    "        )\n",
    "\n",
    "        json_stats= {\n",
    "            \"matches\": re.sub('\\s+','',self.first_class_batting_stats.matches.extract()[0]),\n",
    "            \"inns\":re.sub('\\s+','',self.first_class_batting_stats.inns.extract()[0]),\n",
    "            \"not_outs\":re.sub('\\s+','',self.first_class_batting_stats.not_outs.extract()[0]),\n",
    "            \"runs\":re.sub('\\s+','',self.first_class_batting_stats.runs.extract()[0]),\n",
    "            \"highest\":re.sub('\\s+','',self.first_class_batting_stats.highest.extract()[0]),\n",
    "            \"average\":re.sub('\\s+','',self.first_class_batting_stats.average.extract()[0]),\n",
    "            \"balls_faced\":re.sub('\\s+','',self.first_class_batting_stats.balls_faced.extract()[0]),\n",
    "            \"strike_rate\":re.sub('\\s+','',self.first_class_batting_stats.strike_rate.extract()[0]),\n",
    "            \"hundreds\":re.sub('\\s+','',self.first_class_batting_stats.hundreds.extract()[0]),\n",
    "            \"fifties\":re.sub('\\s+','',self.first_class_batting_stats.fifties.extract()[0]),\n",
    "            \"boundaries\":re.sub('\\s+','',self.first_class_batting_stats.boundaries.extract()[0]),\n",
    "            \"sixes\":re.sub('\\s+','',self.first_class_batting_stats.sixes.extract()[0]),\n",
    "            \"catches_taken\":re.sub('\\s+','',self.first_class_batting_stats.catches_taken.extract()[0]),\n",
    "            \"stumps\":re.sub('\\s+','',self.first_class_batting_stats.stumpings_made.extract()[0])\n",
    "        }\n",
    "\n",
    "        yield {\n",
    "            \"name\": name.extract()[0].replace('\\n',''),\n",
    "            \"url\": response.meta['url'].replace('\\n',''),\n",
    "            \"born\": dob_place.extract()[0].replace('\\n',''),\n",
    "            \"teams\": teams.extract()[0].replace('\\n',''),\n",
    "            \"first_class\": json_stats\n",
    "        }\n",
    "        \n",
    "        \n",
    "process = CrawlerProcess()\n",
    "m = []\n",
    "m.append(process.crawl(CricInfoPlayerInfoSpider))\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
